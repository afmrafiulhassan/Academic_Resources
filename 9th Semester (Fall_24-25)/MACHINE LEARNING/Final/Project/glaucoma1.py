# -*- coding: utf-8 -*-
"""Glaucoma1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12sZ6IsOISo2JvIb26O1Uy3O-hp2sxo5O
"""

!pip install kaggle

!kaggle datasets download -d harishprabhu/glucoma-oct-images

!unzip glucoma-oct-images.zip -d ./glucoma-oct-images



# Importing necessary libraries

import numpy as np
import os
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from random import randint

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping

def load_from_directory(data_dir):

    images = []
    labels = []

    # Loop throufh each folder in the dataset directory
    for folder_name in os.listdir(data_dir):
        folder_path = os.path.join(data_dir, folder_name)

        # Loop through each image in the folder
        for img_name in os.listdir(folder_path):
            img_path = os.path.join(folder_path, img_name)

            # Read the image using OpenCV
            img = cv2.imread(img_path)

            # Check if image are valid or not
            if img is not None:

                # Convertion from BGR to GrayScale, RGB
#                 img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Resize the image to target size (224, 224)
                img = cv2.resize(img, (224, 224), interpolation = cv2.INTER_AREA)

                # Append to the empty list
                images.append(img)
                labels.append(folder_name)

            else:
                print(f"Invalid image exists: {img_path}")

    return images, labels


# Data Directories:
data_dir = '/content/glucoma-oct-images/acrima'
images, labels = load_from_directory(data_dir)

# Images to Array
images = np.array(images)
labels = np.array(labels)

# display Dimensions
print(images.shape)
print(labels.shape)

# Splitting Normal and Advanced Glaucoma Images
normal_glaucoma = images[labels == 'normal_glaucoma']
advance_glaucoma = images[labels == 'advance_glaucoma']

# Display Dimensions
print(f'Dimension of Normal Glaucoma data: {normal_glaucoma.shape}')
print(f'Dimension of Advance Glaucoma data: {advance_glaucoma.shape}')

def plot_random_images(data, label):
    f, ax = plt.subplots(5, 5)
    f.subplots_adjust(0,0,3,3)

    for i in range(0,5,1):
        for j in range(0,5,1):
            rnd_num = randint(0, len(data))
            ax[i, j].imshow(data[rnd_num])
            ax[i, j].set_title(label[rnd_num])
            ax[i, j].axis('off')

plot_random_images(images, labels)

def preprocess_and_enhance_images(images):
    processed_images = []

    for image in images:
        # Noise Reduction
        blurred_image = cv2.GaussianBlur(image, (3, 3), 0)

        # Edge Detection (Canny)
        edges = cv2.Canny(image, 50, 100)

        # Sharpening (Unsharp Masking)
        unsharp_image = cv2.addWeighted(image, 1.0, blurred_image, -0.5, 0)

        processed_images.append(unsharp_image)

    # Convert the list of processed images to an array
    processed_images_array = np.array(processed_images)

    return processed_images_array


images = preprocess_and_enhance_images(images)
plot_random_images(images, labels)

def data_imbalance_check(label):
    # Count the number of samples for each class
    unique_labels, class_counts = np.unique(label, return_counts=True)

    # Plot the class distribution
    plt.figure(figsize=(8, 6))
    plt.bar(unique_labels, class_counts, color='skyblue')
    plt.xlabel('Class')
    plt.ylabel('Number of Samples')
    plt.title('Class Distribution')
    plt.xticks(unique_labels)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

    # Check for class imbalance
    if len(unique_labels) < 2 or np.min(class_counts) / np.max(class_counts) < 0.5:
        return "The dataset is imbalanced."
    else:
        return "The dataset is balanced."

data_imbalance_check(labels)

train_images = images / 255
print(train_images.shape)

train_labels = np.array([0 if labels == 'normal_glaucoma' else 1 for labels in labels])
print(train_labels.shape)

x_train, x_test, y_train, y_test = train_test_split(train_images, train_labels, test_size = 0.3, random_state = 42)

# Display Dimensions
print(f'X_train: {x_train.shape}')
print(f'X_test: {x_test.shape}')
print(f'y_train: {y_train.shape}')
print(f'y_test: {y_test.shape}')

#print(x_train[10].shape)

# Setting value of K (Number of classes to classify)
k = len(set(train_labels))
print(k)

i = Input(shape = x_train[0].shape)

x = Conv2D(64, (3,3), strides = 2, activation = 'relu')(i)
x = Dropout(0.2)(x)
x = Conv2D(64, (3,3), strides = 2, activation = 'relu')(x)
x = MaxPooling2D((2,2))(x)

# x = Dropout(0.2)(x)
x = MaxPooling2D((3,3))(x)

x = Conv2D(64, (3,3), strides = 2, activation = 'relu')(x)
x = Dropout(0.25)(x)
x = MaxPooling2D((3,3))(x)

x = Flatten()(x)

x = Dense(128, activation = 'relu', kernel_regularizer = regularizers.l2(0.02))(x)
x = Dropout(0.5)(x)
x = Dense(k, activation = 'softmax')(x)

model = Model(i, x)

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

model.summary()

# history = model.fit(x_train, y_train , validation_data = (x_test, y_test), epochs = 100,  batch_size = 60, callbacks = [early_stopping])
history = model.fit(x_train, y_train , validation_data = (x_test, y_test), epochs = 60,  batch_size = 60)

# Create a feature extractor model
feature_extractor = Model(inputs=model.input, outputs=model.get_layer('flatten').output)

# Extract features for training and testing data
train_features = feature_extractor.predict(x_train)
test_features = feature_extractor.predict(x_test)

print("Extracted Train Features Shape:", train_features.shape)
print("Extracted Test Features Shape:", test_features.shape)



import matplotlib.pyplot as plt

# Visualize the first two PCA components
plt.figure(figsize=(8, 6))
plt.scatter(train_features_pca[:, 0], train_features_pca[:, 1], c=y_train, cmap='viridis', alpha=0.7)
plt.colorbar(label="Class")
plt.title("PCA Visualization of Selected Features")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

from sklearn.decomposition import PCA

# Reduce dimensionality using PCA
pca = PCA(n_components=50)  # Keeping the top 50 principal components
train_features_pca = pca.fit_transform(train_features)
test_features_pca = pca.transform(test_features)

print("PCA Reduced Train Features Shape:", train_features_pca.shape)
print("PCA Reduced Test Features Shape:", test_features_pca.shape)

val_loss, val_accuracy = model.evaluate(x_test, y_test)
print("Validation Loss:", val_loss)
print("Validation Accuracy:", val_accuracy)